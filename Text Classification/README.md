# ğŸ§  Unified LLM Text Classification Pipeline

A modular, end-to-end project for text classification using Large Language Models (LLMs) â€” supporting **fine-tuning**, **feature extraction**, **zero-shot**, and **few-shot prompting** in a single unified pipeline.

---

## âœ¨ Features

âœ… Fine-tuning with Hugging Face Transformers  
âœ… Feature extraction using LLM embeddings + classical ML  
âœ… Zero-shot classification via NLI models  
âœ… Few-shot prompting via OpenAI / instruct models  
âœ… Unified command-line interface  
âœ… Ready for deployment with Gradio

---

## ğŸ“‚ Project Structure

```

text\_classification\_llm/
â”œâ”€â”€ config.py
â”œâ”€â”€ run\_pipeline.py            # Entry point for all modes
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                # Load and tokenize datasets
â”‚   â”œâ”€â”€ features.py            # Extract LLM embeddings
â”‚   â”œâ”€â”€ train\_features.py      # Train ML on embeddings
â”‚   â”œâ”€â”€ train\_finetune.py      # Fine-tune transformer models
â”‚   â”œâ”€â”€ zero\_shot.py           # Zero-shot via BART/Roberta NLI
â”‚   â”œâ”€â”€ few\_shot.py            # Few-shot prompting via GPT
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation helpers
â”œâ”€â”€ models/                    # Saved models
â”œâ”€â”€ outputs/                   # Logs, plots, reports
â”œâ”€â”€ app/
â”‚   â””â”€â”€ gradio\_app.py          # Web UI (optional)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“Š Supported Classification Modes

| Mode             | Description                                         | File             |
|------------------|-----------------------------------------------------|------------------|
| âœ… Fine-Tuning     | Train a transformer model on labeled data          | `train_finetune.py` |
| âœ… Feature-Based   | Extract embeddings â†’ ML classifier                 | `train_features.py` |
| âœ… Zero-Shot       | Use NLI model to classify without training         | `zero_shot.py`      |
| âœ… Few-Shot        | Prompt LLM with examples â†’ predict class           | `few_shot.py`       |

---


## â–¶ï¸ Run Any Mode

```bash
# Fine-tune BERT on dataset
python run_pipeline.py --mode finetune

# Use embeddings + Logistic Regression
python run_pipeline.py --mode feature

# Use zero-shot classification (no training)
python run_pipeline.py --mode zero

# Use few-shot prompting (OpenAI GPT)
python run_pipeline.py --mode fewshot
```

---

## ğŸ“ˆ Evaluation

* Accuracy, F1-score (feature & finetune modes)
* Per-class metrics
* Confusion matrix in `outputs/`

---

## ğŸ§ª Example Dataset

Uses [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) by default.

Classes:

```
["sadness", "joy", "love", "anger", "fear", "surprise"]
```

You can change dataset in `config.py` or plug in your own.

---

## ğŸŒ Optional: Run Web App

```bash
python app/gradio_app.py
```

Then visit `http://localhost:7860/` to test your model live.

---

## ğŸ§  Few-Shot Prompting Logic

Uses OpenAI Chat models (GPT-3.5, GPT-4) to classify text by example:

**Example Prompt:**

```
Classify the following sentence into one of: [joy, sadness, anger, fear, love, surprise]

Example: I'm feeling great today!
Label: joy

Example: Why is everything so hard today?
Label: sadness

Example: I just got promoted!
Label:
```

---


## ğŸ§  Credits

Built with â¤ï¸ using Hugging Face Transformers, OpenAI, and Scikit-Learn.

---

## ğŸ“„ License

MIT License.
