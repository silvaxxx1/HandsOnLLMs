# ğŸ§  Hands-On LLMs: Fine-Tuning, Prompt Engineering, RAG & Agents

Welcome to **HandsOnLLM**, a modular, real-world project suite for mastering **Large Language Models (LLMs)** through practical use cases. Each subproject demonstrates a different aspect of working with LLMs â€” from prompt engineering and fine-tuning to lightweight local RAG and agent workflows.

---

## ğŸ“¦ Repository Structure

```text
Hands_on_LLM/
â”‚
â”œâ”€â”€ Text Classification/     # Project 1 - Classify text with fine-tuning & prompting
â”œâ”€â”€ Local RAG/               # Project 2 - Build a local Retrieval-Augmented Generator using TinyLlama + llama.cpp
â”œâ”€â”€ notebooks/               # Exploratory notebooks for each module
â”œâ”€â”€ requirements.txt         # Dependencies for all projects
â”œâ”€â”€ pyproject.toml           # Project metadata (optional)
â”œâ”€â”€ main.py                  # Entry point for consolidated runs (WIP)
â”œâ”€â”€ README.md                # You're here!
````

---

## ğŸ”§ Tech Stack & Core Libraries

| Category                   | Tools & Frameworks                               |
| -------------------------- | ------------------------------------------------ |
| LLMs & Fine-Tuning         | `transformers`, `trl`, `peft`, `bitsandbytes`    |
| Embeddings & Search        | `sentence-transformers`, `faiss`, `annoy`        |
| Prompt Engineering         | `langchain`, `setfit`                            |
| Local LLMs                 | `llama.cpp`, `llama-cpp-python`, `TinyLlama`     |
| Evaluation & Visualization | `sklearn`, `evaluate`, `matplotlib`, `nltk`      |
| Notebook & Utilities       | `jupyterlab`, `pandas`, `datasets`, `ipywidgets` |

---

## ğŸ“ Projects Overview

### ğŸŸ© 1. Text Classification

ğŸ“‚ `Text Classification/`

* **Use Cases**: Zero-shot, few-shot, fine-tuned classification
* **Pipelines**:

  * Preprocessing & Feature Extraction
  * Sentence Embedding
  * Fine-tuning transformer models
  * Prompt-based zero-shot prediction
* ğŸ“’ Notebooks:

  * `01_eda.ipynb`, `04_finetune.ipynb`, `05_prompt.ipynb`
* âœ… Output: `finetuned/` model ready for inference

---

### ğŸŸ¨ 2. Local RAG with TinyLlama + llama.cpp

ğŸ“‚ `Local RAG/`

* **Goal**: Deploy a **lightweight, fast** Retrieval-Augmented Generation (RAG) pipeline that works **entirely offline**, even on **CPU-only systems**
* **Key Components**:

  * PDF parsing â†’ chunking â†’ embedding (MiniLM, MPNet, Paraphrase-MiniLM)
  * Local similarity search (FAISS)
  * Contextual prompt construction
  * Inference using **TinyLlama-1.1B** via **llama.cpp**
* ğŸ“¦ Assets:

  * Precomputed embeddings: `.csv` / `.pkl`
  * Local quantized `.gguf` LLM
* ğŸ“’ Notebook: `notebooks/local_Rag.ipynb`, `simple-local-rag.ipynb`

#### ğŸ” RAG Pipeline (Simplified Diagram)

```text
+-------------+    +--------------+    +----------------+    +---------------+
|  PDF Input  | â†’  |   Chunking   | â†’  |  Vector Search  | â†’ |   TinyLlama    |
+-------------+    +--------------+    +----------------+    +---------------+
       â†‘                                                        â†“
    User Query â†----------- Contextual Prompt Injection --------+
```

---


## ğŸš€ Quick Start

### Set up Environment (with `uv`)

```bash
uv venv .venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

---

## ğŸ¯ Goals & Philosophy

* ğŸ’¡ **Real-World Scenarios**: Practical over theoretical
* ğŸ” **Reusable Components**: Modular design for pipelines and prompts
* ğŸ§ª **Testable Code**: Designed for clarity and debugging
* ğŸ”Œ **Low-Resource Friendly**: Run on CPU-only or minimal RAM
* ğŸ” **Explorability First**: Every step has an accompanying notebook

---

## ğŸ¤ Contributions

We welcome pull requests, suggestions, and new modules!
Create an issue or fork the repo and open a PR.

---

## ğŸ“„ License

MIT License Â© 2025 **Ex Machina**

```

